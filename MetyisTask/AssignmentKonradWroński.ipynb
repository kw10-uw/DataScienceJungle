{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Technical Assigment\n",
    "### Author: Konrad Wronski\n",
    "Description: \n",
    "Technical assignment description\n",
    "You are a data engineer working for a retail company. The company’s database handles large\n",
    "volumes of data. You are tasked with creating a datalake in ADLS for reporting purposes.\n",
    "For simplicity, instead of tables, assume files are in a source folder “Sales_Data” in csv format and\n",
    "instead of ADLS, target location is “cleansed” folder (Create it yourself).\n",
    "Source folder contains monthly sales data files.\n",
    "1. Design the process to read all the files from source folder using PySpark, combine them\n",
    "as a single file, and write it to the cleansed folder.\n",
    "2. Which file format would you choose to write in the cleansed folder and why?\n",
    "3. Mention data partitioning strategy you would propose for this table and justify your choice\n",
    "of partitioning method.\n",
    "4. Additionally, outline the steps you would take to implement this partitioning strategy,\n",
    "considering both the technical aspects and potential challenges.\n",
    "Note: Process all the files in a single run. Ensure that there is no data duplication.\n",
    "[Hint: Make use of Window function for deduplication]\n",
    "### Expected results of assignment:\n",
    "- A notebook or python script.\n",
    "- A separate documentation file with a brief explanation of the approach, data exploration,\n",
    "assumptions/considerations, and instructions on how to run the application (if any).\n",
    "- Output dataset in cleansed folder in your preferred file format.\n",
    "- Data quality checks (like input/output dataset validation)\n",
    "### Metyis development guidelines\n",
    "- We appreciate a combination of Software and Data Engineering good practices.\n",
    "- Proper logging and exception handling\n",
    "### Evaluation criteria for results of technical assignment\n",
    "We use following criteria to evaluate results:\n",
    "- Well-structured code: we expect maintainability, readability.\n",
    "- Scalability: Should be able to handle high volumes of data.\n",
    "- Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initializing Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SalesETL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/Users/konradwronski/Desktop/Projects/Grind/DataScienceJungle/MetyisTask/Sales_Data\"\n",
    "endpoint = \"/Users/konradwronski/Desktop/Projects/Grind/DataScienceJungle/MetyisTask/Cleansed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Checking the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header = True, inferSchema = True).csv(\"/Users/konradwronski/Desktop/Projects/Grind/DataScienceJungle/MetyisTask/Sales_Data/Sales_April_2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  176558|USB-C Charging Cable|               2|     11.95|04/19/19 08:46|917 1st St, Dalla...|\n",
      "|    NULL|                NULL|            NULL|      NULL|          NULL|                NULL|\n",
      "|  176559|Bose SoundSport H...|               1|     99.99|04/07/19 22:30|682 Chestnut St, ...|\n",
      "|  176560|        Google Phone|               1|     600.0|04/12/19 14:38|669 Spruce St, Lo...|\n",
      "|  176560|    Wired Headphones|               1|     11.99|04/12/19 14:38|669 Spruce St, Lo...|\n",
      "|  176561|    Wired Headphones|               1|     11.99|04/30/19 09:27|333 8th St, Los A...|\n",
      "|  176562|USB-C Charging Cable|               1|     11.95|04/29/19 13:03|381 Wilson St, Sa...|\n",
      "|  176563|Bose SoundSport H...|               1|     99.99|04/02/19 07:46|668 Center St, Se...|\n",
      "|  176564|USB-C Charging Cable|               1|     11.95|04/12/19 10:58|790 Ridge St, Atl...|\n",
      "|  176565|  Macbook Pro Laptop|               1|    1700.0|04/24/19 10:38|915 Willow St, Sa...|\n",
      "|  176566|    Wired Headphones|               1|     11.99|04/08/19 14:05|83 7th St, Boston...|\n",
      "|  176567|        Google Phone|               1|     600.0|04/18/19 17:18|444 7th St, Los A...|\n",
      "|  176568|Lightning Chargin...|               1|     14.95|04/15/19 12:18|438 Elm St, Seatt...|\n",
      "|  176569|27in 4K Gaming Mo...|               1|    389.99|04/16/19 19:23|657 Hill St, Dall...|\n",
      "|  176570|AA Batteries (4-p...|               1|      3.84|04/22/19 15:09|186 12th St, Dall...|\n",
      "|  176571|Lightning Chargin...|               1|     14.95|04/19/19 14:29|253 Johnson St, A...|\n",
      "|  176572|Apple Airpods Hea...|               1|     150.0|04/04/19 20:30|149 Dogwood St, N...|\n",
      "|  176573|USB-C Charging Cable|               1|     11.95|04/27/19 18:41|214 Chestnut St, ...|\n",
      "|  176574|        Google Phone|               1|     600.0|04/03/19 19:42|20 Hill St, Los A...|\n",
      "|  176574|USB-C Charging Cable|               1|     11.95|04/03/19 19:42|20 Hill St, Los A...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty Data Frame to store merged files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesSchema = StructType([StructField('Order ID',\n",
    "                                  IntegerType(), True),\n",
    "                    StructField('Product',\n",
    "                                StringType(), True),\n",
    "                    StructField('Quantity Ordered',\n",
    "                                IntegerType(), True),\n",
    "                    StructField('Price Each',\n",
    "                                DoubleType(), True),\n",
    "                    StructField('Order Date',\n",
    "                                StringType(), True),\n",
    "                    StructField('Purchase Address',\n",
    "                                StringType(), True)\n",
    "                    ])\n",
    "sales = spark.createDataFrame(data = [], schema = salesSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file, base):\n",
    "    '''Function loading and merging all of the datasets/tables'''\n",
    "    dfFile = spark.read.options(header = True, inferSchema = True).csv(file)\n",
    "    base = base.unionByName(dfFile)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through all of the files in the Sales Data folder to merge them into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File b'Sales_December_2019.csv' has been loaded and appended to the main file. Current Length of the file 25117\n",
      "File b'Sales_April_2019.csv' has been loaded and appended to the main file. Current Length of the file 43500\n",
      "File b'Sales_February_2019.csv' has been loaded and appended to the main file. Current Length of the file 55536\n",
      "File b'Sales_March_2019.csv' has been loaded and appended to the main file. Current Length of the file 70762\n",
      "File b'Sales_August_2019.csv' has been loaded and appended to the main file. Current Length of the file 82773\n",
      "File b'Sales_May_2019.csv' has been loaded and appended to the main file. Current Length of the file 99408\n",
      "File b'Sales_November_2019.csv' has been loaded and appended to the main file. Current Length of the file 117069\n",
      "File b'Sales_October_2019.csv' has been loaded and appended to the main file. Current Length of the file 137448\n",
      "File b'Sales_January_2019.csv' has been loaded and appended to the main file. Current Length of the file 147171\n",
      "File b'Sales_September_2019.csv' has been loaded and appended to the main file. Current Length of the file 158857\n",
      "File b'Sales_July_2019.csv' has been loaded and appended to the main file. Current Length of the file 173228\n",
      "File b'Sales_June_2019.csv' has been loaded and appended to the main file. Current Length of the file 186850\n"
     ]
    }
   ],
   "source": [
    "directory = os.fsencode(source)\n",
    "\n",
    "for file in os.listdir(directory): \n",
    "    filename = os.path.join(source, os.fsdecode(file))\n",
    "    sales = load(filename, sales)\n",
    "    print(f\"File {file} has been loaded and appended to the main file. Current Length of the file {sales.count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
