{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Technical Assigment\n",
    "### Author: Konrad Wronski\n",
    "Description: \n",
    "Technical assignment description\n",
    "You are a data engineer working for a retail company. The company’s database handles large\n",
    "volumes of data. You are tasked with creating a datalake in ADLS for reporting purposes.\n",
    "For simplicity, instead of tables, assume files are in a source folder “Sales_Data” in csv format and\n",
    "instead of ADLS, target location is “cleansed” folder (Create it yourself).\n",
    "Source folder contains monthly sales data files.\n",
    "1. Design the process to read all the files from source folder using PySpark, combine them\n",
    "as a single file, and write it to the cleansed folder.\n",
    "2. Which file format would you choose to write in the cleansed folder and why?\n",
    "3. Mention data partitioning strategy you would propose for this table and justify your choice\n",
    "of partitioning method.\n",
    "4. Additionally, outline the steps you would take to implement this partitioning strategy,\n",
    "considering both the technical aspects and potential challenges.\n",
    "Note: Process all the files in a single run. Ensure that there is no data duplication.\n",
    "[Hint: Make use of Window function for deduplication]\n",
    "### Expected results of assignment:\n",
    "- A notebook or python script.\n",
    "- A separate documentation file with a brief explanation of the approach, data exploration,\n",
    "assumptions/considerations, and instructions on how to run the application (if any).\n",
    "- Output dataset in cleansed folder in your preferred file format.\n",
    "- Data quality checks (like input/output dataset validation)\n",
    "### Metyis development guidelines\n",
    "- We appreciate a combination of Software and Data Engineering good practices.\n",
    "- Proper logging and exception handling\n",
    "### Evaluation criteria for results of technical assignment\n",
    "We use following criteria to evaluate results:\n",
    "- Well-structured code: we expect maintainability, readability.\n",
    "- Scalability: Should be able to handle high volumes of data.\n",
    "- Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/mckmrfpx3yn1g1wj1m8cxm000000gn/T/ipykernel_92932/2147531606.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
