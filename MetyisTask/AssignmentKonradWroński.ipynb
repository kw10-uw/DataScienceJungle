{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Technical Assigment\n",
    "### Author: Konrad Wronski\n",
    "Description: \n",
    "Technical assignment description\n",
    "You are a data engineer working for a retail company. The company’s database handles large\n",
    "volumes of data. You are tasked with creating a datalake in ADLS for reporting purposes.\n",
    "For simplicity, instead of tables, assume files are in a source folder “Sales_Data” in csv format and\n",
    "instead of ADLS, target location is “cleansed” folder (Create it yourself).\n",
    "Source folder contains monthly sales data files.\n",
    "1. Design the process to read all the files from source folder using PySpark, combine them\n",
    "as a single file, and write it to the cleansed folder.\n",
    "2. Which file format would you choose to write in the cleansed folder and why?\n",
    "3. Mention data partitioning strategy you would propose for this table and justify your choice\n",
    "of partitioning method.\n",
    "4. Additionally, outline the steps you would take to implement this partitioning strategy,\n",
    "considering both the technical aspects and potential challenges.\n",
    "Note: Process all the files in a single run. Ensure that there is no data duplication.\n",
    "[Hint: Make use of Window function for deduplication]\n",
    "### Expected results of assignment:\n",
    "- A notebook or python script.\n",
    "- A separate documentation file with a brief explanation of the approach, data exploration,\n",
    "assumptions/considerations, and instructions on how to run the application (if any).\n",
    "- Output dataset in cleansed folder in your preferred file format.\n",
    "- Data quality checks (like input/output dataset validation)\n",
    "### Metyis development guidelines\n",
    "- We appreciate a combination of Software and Data Engineering good practices.\n",
    "- Proper logging and exception handling\n",
    "### Evaluation criteria for results of technical assignment\n",
    "We use following criteria to evaluate results:\n",
    "- Well-structured code: we expect maintainability, readability.\n",
    "- Scalability: Should be able to handle high volumes of data.\n",
    "- Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col,isnan, when, count, to_timestamp, to_date, date_format, row_number, col, year, month\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initializing Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SalesETL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/Users/konradwronski/Desktop/Projects/Grind/DataScienceJungle/MetyisTask/Sales_Data\"\n",
    "endpoint = \"/Users/konradwronski/Desktop/Projects/Grind/DataScienceJungle/MetyisTask/Cleansed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Checking the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header = True, inferSchema = True).csv(\"/Users/konradwronski/Desktop/Projects/Grind/DataScienceJungle/MetyisTask/Sales_Data/Sales_April_2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  176558|USB-C Charging Cable|               2|     11.95|04/19/19 08:46|917 1st St, Dalla...|\n",
      "|    NULL|                NULL|            NULL|      NULL|          NULL|                NULL|\n",
      "|  176559|Bose SoundSport H...|               1|     99.99|04/07/19 22:30|682 Chestnut St, ...|\n",
      "|  176560|        Google Phone|               1|     600.0|04/12/19 14:38|669 Spruce St, Lo...|\n",
      "|  176560|    Wired Headphones|               1|     11.99|04/12/19 14:38|669 Spruce St, Lo...|\n",
      "|  176561|    Wired Headphones|               1|     11.99|04/30/19 09:27|333 8th St, Los A...|\n",
      "|  176562|USB-C Charging Cable|               1|     11.95|04/29/19 13:03|381 Wilson St, Sa...|\n",
      "|  176563|Bose SoundSport H...|               1|     99.99|04/02/19 07:46|668 Center St, Se...|\n",
      "|  176564|USB-C Charging Cable|               1|     11.95|04/12/19 10:58|790 Ridge St, Atl...|\n",
      "|  176565|  Macbook Pro Laptop|               1|    1700.0|04/24/19 10:38|915 Willow St, Sa...|\n",
      "|  176566|    Wired Headphones|               1|     11.99|04/08/19 14:05|83 7th St, Boston...|\n",
      "|  176567|        Google Phone|               1|     600.0|04/18/19 17:18|444 7th St, Los A...|\n",
      "|  176568|Lightning Chargin...|               1|     14.95|04/15/19 12:18|438 Elm St, Seatt...|\n",
      "|  176569|27in 4K Gaming Mo...|               1|    389.99|04/16/19 19:23|657 Hill St, Dall...|\n",
      "|  176570|AA Batteries (4-p...|               1|      3.84|04/22/19 15:09|186 12th St, Dall...|\n",
      "|  176571|Lightning Chargin...|               1|     14.95|04/19/19 14:29|253 Johnson St, A...|\n",
      "|  176572|Apple Airpods Hea...|               1|     150.0|04/04/19 20:30|149 Dogwood St, N...|\n",
      "|  176573|USB-C Charging Cable|               1|     11.95|04/27/19 18:41|214 Chestnut St, ...|\n",
      "|  176574|        Google Phone|               1|     600.0|04/03/19 19:42|20 Hill St, Los A...|\n",
      "|  176574|USB-C Charging Cable|               1|     11.95|04/03/19 19:42|20 Hill St, Los A...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: integer (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: integer (nullable = true)\n",
      " |-- Price Each: double (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty Data Frame to store merged files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesSchema = StructType([StructField('Order ID',\n",
    "                                  IntegerType(), True),\n",
    "                    StructField('Product',\n",
    "                                StringType(), True),\n",
    "                    StructField('Quantity Ordered',\n",
    "                                IntegerType(), True),\n",
    "                    StructField('Price Each',\n",
    "                                DoubleType(), True),\n",
    "                    StructField('Order Date',\n",
    "                                StringType(), True),\n",
    "                    StructField('Purchase Address',\n",
    "                                StringType(), True)\n",
    "                    ])\n",
    "sales = spark.createDataFrame(data = [], schema = salesSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file, base):\n",
    "    '''Function loading and merging all of the datasets/tables'''\n",
    "    dfFile = spark.read.options(header = True).schema(salesSchema).csv(file)\n",
    "    base = base.unionByName(dfFile)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through all of the files in the Sales Data folder to merge them into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File b'Sales_December_2019.csv' has been loaded and appended to the main file. Current Length of the file 25117\n",
      "File b'Sales_April_2019.csv' has been loaded and appended to the main file. Current Length of the file 43500\n",
      "File b'Sales_February_2019.csv' has been loaded and appended to the main file. Current Length of the file 55536\n",
      "File b'Sales_March_2019.csv' has been loaded and appended to the main file. Current Length of the file 70762\n",
      "File b'Sales_August_2019.csv' has been loaded and appended to the main file. Current Length of the file 82773\n",
      "File b'Sales_May_2019.csv' has been loaded and appended to the main file. Current Length of the file 99408\n",
      "File b'Sales_November_2019.csv' has been loaded and appended to the main file. Current Length of the file 117069\n",
      "File b'Sales_October_2019.csv' has been loaded and appended to the main file. Current Length of the file 137448\n",
      "File b'Sales_January_2019.csv' has been loaded and appended to the main file. Current Length of the file 147171\n",
      "File b'Sales_September_2019.csv' has been loaded and appended to the main file. Current Length of the file 158857\n",
      "File b'Sales_July_2019.csv' has been loaded and appended to the main file. Current Length of the file 173228\n",
      "File b'Sales_June_2019.csv' has been loaded and appended to the main file. Current Length of the file 186850\n"
     ]
    }
   ],
   "source": [
    "directory = os.fsencode(source)\n",
    "\n",
    "for file in os.listdir(directory): \n",
    "    filename = os.path.join(source, os.fsdecode(file))\n",
    "    sales = load(filename, sales)\n",
    "    print(f\"File {file} has been loaded and appended to the main file. Current Length of the file {sales.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  295665|  Macbook Pro Laptop|               1|    1700.0|12/30/19 00:01|136 Church St, Ne...|\n",
      "|  295666|  LG Washing Machine|               1|     600.0|12/29/19 07:03|562 2nd St, New Y...|\n",
      "|  295667|USB-C Charging Cable|               1|     11.95|12/12/19 18:21|277 Main St, New ...|\n",
      "|  295668|    27in FHD Monitor|               1|    149.99|12/22/19 15:13|410 6th St, San F...|\n",
      "|  295669|USB-C Charging Cable|               1|     11.95|12/18/19 12:38|43 Hill St, Atlan...|\n",
      "|  295670|AA Batteries (4-p...|               1|      3.84|12/31/19 22:58|200 Jefferson St,...|\n",
      "|  295671|USB-C Charging Cable|               1|     11.95|12/16/19 15:10|928 12th St, Port...|\n",
      "|  295672|USB-C Charging Cable|               2|     11.95|12/13/19 09:29|813 Hickory St, D...|\n",
      "|  295673|Bose SoundSport H...|               1|     99.99|12/15/19 23:26|718 Wilson St, Da...|\n",
      "|  295674|AAA Batteries (4-...|               4|      2.99|12/28/19 11:51|77 7th St, Dallas...|\n",
      "|  295675|USB-C Charging Cable|               2|     11.95|12/13/19 13:52|594 1st St, San F...|\n",
      "|  295676|     ThinkPad Laptop|               1|    999.99|12/28/19 17:19|410 Lincoln St, L...|\n",
      "|  295677|AA Batteries (4-p...|               2|      3.84|12/20/19 19:19|866 Pine St, Bost...|\n",
      "|  295678|AAA Batteries (4-...|               2|      2.99|12/06/19 09:38|187 Lincoln St, D...|\n",
      "|  295679|USB-C Charging Cable|               1|     11.95|12/25/19 09:39|902 2nd St, Dalla...|\n",
      "|  295680|Lightning Chargin...|               1|     14.95|12/01/19 14:30|338 Main St, Aust...|\n",
      "|  295681|        Google Phone|               1|     600.0|12/25/19 12:37|79 Elm St, Boston...|\n",
      "|  295681|USB-C Charging Cable|               1|     11.95|12/25/19 12:37|79 Elm St, Boston...|\n",
      "|  295681|Bose SoundSport H...|               1|     99.99|12/25/19 12:37|79 Elm St, Boston...|\n",
      "|  295681|    Wired Headphones|               1|     11.99|12/25/19 12:37|79 Elm St, Boston...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:=======================================>                (14 + 6) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "|Order ID|Product|Quantity Ordered|Price Each|Order Date|Purchase Address|\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "|     900|    545|             900|       900|       545|             545|\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "|Order ID|Product|Quantity Ordered|Price Each|Order Date|Purchase Address|\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "|NULL    |Product|NULL            |NULL      |Order Date|Purchase Address|\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |Product|NULL            |NULL      |Order Date|Purchase Address|\n",
      "|NULL    |Product|NULL            |NULL      |Order Date|Purchase Address|\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |Product|NULL            |NULL      |Order Date|Purchase Address|\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |Product|NULL            |NULL      |Order Date|Purchase Address|\n",
      "|NULL    |NULL   |NULL            |NULL      |NULL      |NULL            |\n",
      "|NULL    |Product|NULL            |NULL      |Order Date|Purchase Address|\n",
      "|NULL    |Product|NULL            |NULL      |Order Date|Purchase Address|\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "missing_data_rows = sales.filter(\n",
    "    (col(\"Order ID\").isNull()) | (col(\"Product\").isNull()) | \n",
    "    (col(\"Quantity Ordered\").isNull()) | (col(\"Price Each\").isNull()) |\n",
    "    (col(\"Order Date\").isNull()) | (col(\"Purchase Address\").isNull()) |\n",
    "    (isnan(\"Order ID\")) | (isnan(\"Product\")) | \n",
    "    (isnan(\"Quantity Ordered\")) | (isnan(\"Price Each\")) |\n",
    "    (isnan(\"Order Date\")) | (isnan(\"Purchase Address\"))\n",
    ")\n",
    "missing_data_rows.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:=================================>                      (12 + 8) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "|Order ID|Product|Quantity Ordered|Price Each|Order Date|Purchase Address|\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "|       0|      0|               0|         0|         0|               0|\n",
      "+--------+-------+----------------+----------+----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Checking for duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "|  141250|     Vareebadd Phone|               1|     400.0|01/10/19 11:20|471 Center St, Lo...|\n",
      "|  141254|AAA Batteries (4-...|               1|      2.99|01/08/19 11:51|238 Sunset St, Se...|\n",
      "|  141265|Apple Airpods Hea...|               1|     150.0|01/01/19 16:52|853 Ridge St, Bos...|\n",
      "|  141270|    Wired Headphones|               1|     11.99|01/27/19 23:10|469 Hill St, San ...|\n",
      "|  141275|    Wired Headphones|               1|     11.99|01/07/19 16:06|610 Walnut St, Au...|\n",
      "|  141321|Bose SoundSport H...|               1|     99.99|01/10/19 09:07|207 8th St, Los A...|\n",
      "|  141334|USB-C Charging Cable|               1|     11.95|01/15/19 02:05|459 4th St, San F...|\n",
      "|  141338|AAA Batteries (4-...|               1|      2.99|01/29/19 16:19|542 4th St, Dalla...|\n",
      "|  141339|AAA Batteries (4-...|               1|      2.99|01/28/19 12:52|539 Lakeview St, ...|\n",
      "|  141341|            LG Dryer|               1|     600.0|01/13/19 10:09|844 Walnut St, Au...|\n",
      "|  141353|USB-C Charging Cable|               1|     11.95|01/26/19 10:35|870 Cherry St, Ne...|\n",
      "|  141362|Lightning Chargin...|               1|     14.95|01/02/19 22:17|392 Cherry St, At...|\n",
      "|  141364|AA Batteries (4-p...|               1|      3.84|01/01/19 07:46|657 Lincoln St, D...|\n",
      "|  141365|     Vareebadd Phone|               1|     400.0|01/10/19 11:19|20 Dogwood St, Ne...|\n",
      "|  141366|       Flatscreen TV|               1|     300.0|01/17/19 22:34|803 Church St, Se...|\n",
      "|  141367|27in 4K Gaming Mo...|               1|    389.99|01/09/19 10:06|23 13th St, San F...|\n",
      "|  141372|AAA Batteries (4-...|               1|      2.99|01/26/19 10:07|134 Hickory St, P...|\n",
      "|  141383|AAA Batteries (4-...|               1|      2.99|01/08/19 10:01|418 11th St, Bost...|\n",
      "|  141394|              iPhone|               1|     700.0|01/06/19 16:54|534 12th St, San ...|\n",
      "|  141410|    27in FHD Monitor|               1|    149.99|01/03/19 22:02|890 6th St, Dalla...|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "185688"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"Order ID\", \"Product\", \"Quantity Ordered\", \"Price Each\", \"Purchase Address\") \\\n",
    "                    .orderBy(col(\"Order Date\").desc())\n",
    "\n",
    "# Apply row_number for deduplication\n",
    "sales_no_dup = sales.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Keep only the first row for each partition\n",
    "sales_no_dup = sales_no_dup.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "sales_no_dup.show()\n",
    "\n",
    "sales_no_dup.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Transforming Order Date column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing column data type to timestamp then spliting into two columns: Timestamp and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.withColumn(\"Order Date\", to_timestamp(\"Order Date\", \"MM/dd/yy HH:mm\"))\n",
    "sales = sales.withColumn(\"Order Timestamp\", date_format(\"Order Date\", \"HH:mm\"))\n",
    "sales = sales.withColumn(\"Order Date\", to_date(\"Order Date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+----------+--------------------+---------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|Order Date|    Purchase Address|Order Timestamp|\n",
      "+--------+--------------------+----------------+----------+----------+--------------------+---------------+\n",
      "|  295665|  Macbook Pro Laptop|               1|    1700.0|2019-12-30|136 Church St, Ne...|          00:01|\n",
      "|  295666|  LG Washing Machine|               1|     600.0|2019-12-29|562 2nd St, New Y...|          07:03|\n",
      "|  295667|USB-C Charging Cable|               1|     11.95|2019-12-12|277 Main St, New ...|          18:21|\n",
      "|  295668|    27in FHD Monitor|               1|    149.99|2019-12-22|410 6th St, San F...|          15:13|\n",
      "|  295669|USB-C Charging Cable|               1|     11.95|2019-12-18|43 Hill St, Atlan...|          12:38|\n",
      "|  295670|AA Batteries (4-p...|               1|      3.84|2019-12-31|200 Jefferson St,...|          22:58|\n",
      "|  295671|USB-C Charging Cable|               1|     11.95|2019-12-16|928 12th St, Port...|          15:10|\n",
      "|  295672|USB-C Charging Cable|               2|     11.95|2019-12-13|813 Hickory St, D...|          09:29|\n",
      "|  295673|Bose SoundSport H...|               1|     99.99|2019-12-15|718 Wilson St, Da...|          23:26|\n",
      "|  295674|AAA Batteries (4-...|               4|      2.99|2019-12-28|77 7th St, Dallas...|          11:51|\n",
      "|  295675|USB-C Charging Cable|               2|     11.95|2019-12-13|594 1st St, San F...|          13:52|\n",
      "|  295676|     ThinkPad Laptop|               1|    999.99|2019-12-28|410 Lincoln St, L...|          17:19|\n",
      "|  295677|AA Batteries (4-p...|               2|      3.84|2019-12-20|866 Pine St, Bost...|          19:19|\n",
      "|  295678|AAA Batteries (4-...|               2|      2.99|2019-12-06|187 Lincoln St, D...|          09:38|\n",
      "|  295679|USB-C Charging Cable|               1|     11.95|2019-12-25|902 2nd St, Dalla...|          09:39|\n",
      "|  295680|Lightning Chargin...|               1|     14.95|2019-12-01|338 Main St, Aust...|          14:30|\n",
      "|  295681|        Google Phone|               1|     600.0|2019-12-25|79 Elm St, Boston...|          12:37|\n",
      "|  295681|USB-C Charging Cable|               1|     11.95|2019-12-25|79 Elm St, Boston...|          12:37|\n",
      "|  295681|Bose SoundSport H...|               1|     99.99|2019-12-25|79 Elm St, Boston...|          12:37|\n",
      "|  295681|    Wired Headphones|               1|     11.99|2019-12-25|79 Elm St, Boston...|          12:37|\n",
      "+--------+--------------------+----------------+----------+----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Data into new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.withColumn(\"year\", year(col(\"Order date\")))\n",
    "sales = sales.withColumn(\"month\", month(col(\"Order date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 18:53:46 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/02/17 18:53:46 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "25/02/17 18:53:46 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/02/17 18:53:46 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/02/17 18:53:46 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "25/02/17 18:53:47 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/02/17 18:53:47 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_count = sales.count()\n",
    "output_count = spark.read.parquet(endpoint).count()\n",
    "assert input_count == output_count, \"Data loss occurred!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "- Top Selling Products\n",
    "- Sales trend - line graph of sales in each month\n",
    "- Top buying spots\n",
    "- Day of the week - when sales are the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
